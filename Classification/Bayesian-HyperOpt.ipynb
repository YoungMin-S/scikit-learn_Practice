{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bb7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# -10 ~ 10까지 1간격을 가지는 입력 변수 x 집합값과 -15 ~ 15까지 1간격을 가지는 입력 변수 y 집합값 설정.\n",
    "search_space = {'x' : hp.quniform('x',-10,10,1),'y': hp.quniform('y',-15,15,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6fedd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': <hyperopt.pyll.base.Apply at 0x12fb94dd0>,\n",
       " 'y': <hyperopt.pyll.base.Apply at 0x12fd9ca10>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c481b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# 목적 함수를 생성. 입력 변수값과 입력 변수 검색 범위를 가지는 딕셔너리를 인자로 받고, 특정 값을 반환\n",
    "def objective_func(search_space):\n",
    "    x = search_space['x']\n",
    "    y = search_space['y']\n",
    "    retval = x**2 - 20*y\n",
    "    \n",
    "    return retval # return {'loss':retval,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7450a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 5/5 [00:00<00:00, 977.69trial/s, best loss: -224.0]\n",
      "best :  {'x': -4.0, 'y': 12.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin,tpe,Trials\n",
    "import numpy as np\n",
    "\n",
    "# 입력 결괏값을 저장한 Trials 객체 값 생성\n",
    "trial_val = Trials()\n",
    "\n",
    "# 목적 함수의 최솟값을 반환하는 최적 입력 변숫값을 5번의 입력값 시도(max_evals=5)로 찾아냄.\n",
    "best_01 = fmin(fn=objective_func,space=search_space,algo=tpe.suggest,max_evals=5,trials=trial_val\n",
    "               ,rstate=np.random.default_rng(seed=0))\n",
    "\n",
    "print('best : ',best_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42901b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 20/20 [00:00<00:00, 973.90trial/s, best loss: -296.0]\n",
      "best : {'x': 2.0, 'y': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trial_val = Trials()\n",
    "# max_evals=20으로 재 테스트 \n",
    "best_02 = fmin(fn=objective_func,space=search_space,algo=tpe.suggest,max_evals=20,trials=trial_val\n",
    "              ,rstate=np.random.default_rng(seed=0))\n",
    "\n",
    "print('best :',best_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8644000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x158888c90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39b0e",
   "metadata": {},
   "source": [
    "+ HyperOpt 수행 시 적용된 입력 값들과 목적 함수 반환값 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92566ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': -64.0, 'status': 'ok'}, {'loss': -184.0, 'status': 'ok'}, {'loss': 56.0, 'status': 'ok'}, {'loss': -224.0, 'status': 'ok'}, {'loss': 61.0, 'status': 'ok'}, {'loss': -296.0, 'status': 'ok'}, {'loss': -40.0, 'status': 'ok'}, {'loss': 281.0, 'status': 'ok'}, {'loss': 64.0, 'status': 'ok'}, {'loss': 100.0, 'status': 'ok'}, {'loss': 60.0, 'status': 'ok'}, {'loss': -39.0, 'status': 'ok'}, {'loss': 1.0, 'status': 'ok'}, {'loss': -164.0, 'status': 'ok'}, {'loss': 21.0, 'status': 'ok'}, {'loss': -56.0, 'status': 'ok'}, {'loss': 284.0, 'status': 'ok'}, {'loss': 176.0, 'status': 'ok'}, {'loss': -171.0, 'status': 'ok'}, {'loss': 0.0, 'status': 'ok'}]\n"
     ]
    }
   ],
   "source": [
    "# fmin( )에 인자로 들어가는 Trials 객체의 result 속성에 파이썬 리스트로 목적 함수 반환값들이 저장됨\n",
    "# 리스트 내부의 개별 원소는 {'loss':함수 반환값, 'status':반환 상태값} 와 같은 딕셔너리임. \n",
    "print(trial_val.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1ddd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [-6.0, -4.0, 4.0, -4.0, 9.0, 2.0, 10.0, -9.0, -8.0, -0.0, -0.0, 1.0, 9.0, 6.0, 9.0, 2.0, -2.0, -4.0, 7.0, -0.0], 'y': [5.0, 10.0, -2.0, 12.0, 1.0, 15.0, 7.0, -10.0, 0.0, -5.0, -3.0, 2.0, 4.0, 10.0, 3.0, 3.0, -14.0, -8.0, 11.0, -0.0]}\n"
     ]
    }
   ],
   "source": [
    "# Trials 객체의 vals 속성에 {'입력변수명':개별 수행 시마다 입력된 값 리스트} 형태로 저장됨.\n",
    "print(trial_val.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5b66f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x     y  losses\n",
       "0   -6.0   5.0   -64.0\n",
       "1   -4.0  10.0  -184.0\n",
       "2    4.0  -2.0    56.0\n",
       "3   -4.0  12.0  -224.0\n",
       "4    9.0   1.0    61.0\n",
       "5    2.0  15.0  -296.0\n",
       "6   10.0   7.0   -40.0\n",
       "7   -9.0 -10.0   281.0\n",
       "8   -8.0   0.0    64.0\n",
       "9   -0.0  -5.0   100.0\n",
       "10  -0.0  -3.0    60.0\n",
       "11   1.0   2.0   -39.0\n",
       "12   9.0   4.0     1.0\n",
       "13   6.0  10.0  -164.0\n",
       "14   9.0   3.0    21.0\n",
       "15   2.0   3.0   -56.0\n",
       "16  -2.0 -14.0   284.0\n",
       "17  -4.0  -8.0   176.0\n",
       "18   7.0  11.0  -171.0\n",
       "19  -0.0  -0.0     0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# result에서 loss 키값에 해당하는 밸류들을 추출하여 list로 생성.\n",
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "\n",
    "# DataFrame으로 생성\n",
    "result_df = pd.DataFrame({'x':trial_val.vals['x'],\n",
    "                         'y':trial_val.vals['y'],\n",
    "                         'losses': losses\n",
    "                         })\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c5c81",
   "metadata": {},
   "source": [
    "### HyperOpt를 XGBoost 하이퍼 파라미터 튜닝에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec78bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "cancer_df = pd.DataFrame(dataset.data,columns=dataset.feature_names)\n",
    "cancer_df['target'] = dataset.target \n",
    "\n",
    "X_feature = cancer_df.iloc[:,:-1]\n",
    "y_label = cancer_df.iloc[:,-1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_feature,y_label,test_size=0.2)\n",
    "\n",
    "X_tr,X_val,y_tr,y_val = train_test_split(X_train,y_train,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eb0eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# max_depth는 5에서 20까지 1간격으로 min_child_weight는 1에서2까지 1간격으로\n",
    "# colsample_bytreesms 0.5에서 1사이,learning_rate는 0.01에서 0.2 정규 분포된 값으로 검색\n",
    "\n",
    "xgb_search_space = {'max_depth': hp.quniform('max_depth',5,20,1),\n",
    "                    'min_child_weight': hp.quniform('min_child_weight',1,2,1),\n",
    "                    'colsample_bytree': hp.uniform('colsample_bytree',0.5,1),\n",
    "                    'learning_rate': hp.uniform('learning_rate',0.01,0.2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "723d88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# fmin()에서 입력된 search_space값으로 입력된 모든 값은 실수형임. \n",
    "# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함. \n",
    "# 정확도는 높은 수록 더 좋은 수치임. -1* 정확도를 곱해서 큰 정확도 값일 수록 최소가 되도록 변환\n",
    "def objective_func(search_space):\n",
    "    # 수행시간 절약을 위해 n_estimators는 100으로 축소\n",
    "    xgb_clf = XGBClassifier(n_estimators=100,max_depth=int(search_space['max_depth']),\n",
    "                            min_child_weight = int(search_space['min_child_weight']),\n",
    "                            learning_rate=search_space['learning_rate'],\n",
    "                            colsample_bytree = search_space['colsample_bytree'],\n",
    "                            eval_metric='logloss')\n",
    "    \n",
    "    accuracy = cross_val_score(xgb_clf,X_train,y_train,scoring='accuracy',cv=3)\n",
    "    \n",
    "    # accuracy 는 cv=3개수만큼의 정확도 결과를 가지므로 이를 평균해서 반환하되 -1을 곱해줌\n",
    "    \n",
    "    return {'loss':-1*np.mean(accuracy),'status':STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7a0bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 50/50 [00:06<00:00,  7.98trial/s, best loss: -0.9626031137446264]\n",
      "best : {'colsample_bytree': 0.5018690429031908, 'learning_rate': 0.14017335542259288, 'max_depth': 18.0, 'min_child_weight': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin,tpe,Trials\n",
    "\n",
    "trial_val =Trials()\n",
    "best = fmin(fn=objective_func,space=xgb_search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trial_val)\n",
    "print(\"best :\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa26a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree:0.50187, learning_rate:0.14017, max_depth:18, min_child_weight:2\n"
     ]
    }
   ],
   "source": [
    "print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(\n",
    "                        round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),\n",
    "                        int(best['max_depth']), int(best['min_child_weight'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b06e887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f8fe8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.58289\tvalidation_1-logloss:0.59079\n",
      "[1]\tvalidation_0-logloss:0.49685\tvalidation_1-logloss:0.50267\n",
      "[2]\tvalidation_0-logloss:0.42976\tvalidation_1-logloss:0.44140\n",
      "[3]\tvalidation_0-logloss:0.37446\tvalidation_1-logloss:0.39166\n",
      "[4]\tvalidation_0-logloss:0.33154\tvalidation_1-logloss:0.34315\n",
      "[5]\tvalidation_0-logloss:0.29349\tvalidation_1-logloss:0.31291\n",
      "[6]\tvalidation_0-logloss:0.26086\tvalidation_1-logloss:0.27536\n",
      "[7]\tvalidation_0-logloss:0.23340\tvalidation_1-logloss:0.24933\n",
      "[8]\tvalidation_0-logloss:0.20978\tvalidation_1-logloss:0.22844\n",
      "[9]\tvalidation_0-logloss:0.18933\tvalidation_1-logloss:0.20403\n",
      "[10]\tvalidation_0-logloss:0.17244\tvalidation_1-logloss:0.18444\n",
      "[11]\tvalidation_0-logloss:0.15712\tvalidation_1-logloss:0.17196\n",
      "[12]\tvalidation_0-logloss:0.14433\tvalidation_1-logloss:0.15916\n",
      "[13]\tvalidation_0-logloss:0.13294\tvalidation_1-logloss:0.14992\n",
      "[14]\tvalidation_0-logloss:0.12300\tvalidation_1-logloss:0.13821\n",
      "[15]\tvalidation_0-logloss:0.11358\tvalidation_1-logloss:0.12892\n",
      "[16]\tvalidation_0-logloss:0.10556\tvalidation_1-logloss:0.12145\n",
      "[17]\tvalidation_0-logloss:0.09849\tvalidation_1-logloss:0.11564\n",
      "[18]\tvalidation_0-logloss:0.09194\tvalidation_1-logloss:0.10691\n",
      "[19]\tvalidation_0-logloss:0.08634\tvalidation_1-logloss:0.10180\n",
      "[20]\tvalidation_0-logloss:0.08144\tvalidation_1-logloss:0.09785\n",
      "[21]\tvalidation_0-logloss:0.07715\tvalidation_1-logloss:0.09362\n",
      "[22]\tvalidation_0-logloss:0.07310\tvalidation_1-logloss:0.09047\n",
      "[23]\tvalidation_0-logloss:0.06977\tvalidation_1-logloss:0.08403\n",
      "[24]\tvalidation_0-logloss:0.06622\tvalidation_1-logloss:0.08004\n",
      "[25]\tvalidation_0-logloss:0.06300\tvalidation_1-logloss:0.07512\n",
      "[26]\tvalidation_0-logloss:0.05977\tvalidation_1-logloss:0.07313\n",
      "[27]\tvalidation_0-logloss:0.05722\tvalidation_1-logloss:0.07287\n",
      "[28]\tvalidation_0-logloss:0.05466\tvalidation_1-logloss:0.06921\n",
      "[29]\tvalidation_0-logloss:0.05250\tvalidation_1-logloss:0.06856\n",
      "[30]\tvalidation_0-logloss:0.05051\tvalidation_1-logloss:0.06698\n",
      "[31]\tvalidation_0-logloss:0.04857\tvalidation_1-logloss:0.06411\n",
      "[32]\tvalidation_0-logloss:0.04710\tvalidation_1-logloss:0.06232\n",
      "[33]\tvalidation_0-logloss:0.04545\tvalidation_1-logloss:0.06185\n",
      "[34]\tvalidation_0-logloss:0.04386\tvalidation_1-logloss:0.06083\n",
      "[35]\tvalidation_0-logloss:0.04267\tvalidation_1-logloss:0.06275\n",
      "[36]\tvalidation_0-logloss:0.04149\tvalidation_1-logloss:0.06178\n",
      "[37]\tvalidation_0-logloss:0.04039\tvalidation_1-logloss:0.06151\n",
      "[38]\tvalidation_0-logloss:0.03927\tvalidation_1-logloss:0.06045\n",
      "[39]\tvalidation_0-logloss:0.03842\tvalidation_1-logloss:0.06171\n",
      "[40]\tvalidation_0-logloss:0.03759\tvalidation_1-logloss:0.06079\n",
      "[41]\tvalidation_0-logloss:0.03643\tvalidation_1-logloss:0.06110\n",
      "[42]\tvalidation_0-logloss:0.03581\tvalidation_1-logloss:0.06274\n",
      "[43]\tvalidation_0-logloss:0.03482\tvalidation_1-logloss:0.06279\n",
      "[44]\tvalidation_0-logloss:0.03424\tvalidation_1-logloss:0.06125\n",
      "[45]\tvalidation_0-logloss:0.03369\tvalidation_1-logloss:0.06286\n",
      "[46]\tvalidation_0-logloss:0.03287\tvalidation_1-logloss:0.06166\n",
      "[47]\tvalidation_0-logloss:0.03242\tvalidation_1-logloss:0.06161\n",
      "[48]\tvalidation_0-logloss:0.03181\tvalidation_1-logloss:0.05880\n",
      "[49]\tvalidation_0-logloss:0.03104\tvalidation_1-logloss:0.06004\n",
      "[50]\tvalidation_0-logloss:0.03058\tvalidation_1-logloss:0.06019\n",
      "[51]\tvalidation_0-logloss:0.03019\tvalidation_1-logloss:0.05897\n",
      "[52]\tvalidation_0-logloss:0.02953\tvalidation_1-logloss:0.05795\n",
      "[53]\tvalidation_0-logloss:0.02903\tvalidation_1-logloss:0.05766\n",
      "[54]\tvalidation_0-logloss:0.02863\tvalidation_1-logloss:0.05930\n",
      "[55]\tvalidation_0-logloss:0.02830\tvalidation_1-logloss:0.05913\n",
      "[56]\tvalidation_0-logloss:0.02785\tvalidation_1-logloss:0.05812\n",
      "[57]\tvalidation_0-logloss:0.02750\tvalidation_1-logloss:0.05806\n",
      "[58]\tvalidation_0-logloss:0.02717\tvalidation_1-logloss:0.05735\n",
      "[59]\tvalidation_0-logloss:0.02684\tvalidation_1-logloss:0.05682\n",
      "[60]\tvalidation_0-logloss:0.02627\tvalidation_1-logloss:0.05472\n",
      "[61]\tvalidation_0-logloss:0.02590\tvalidation_1-logloss:0.05261\n",
      "[62]\tvalidation_0-logloss:0.02536\tvalidation_1-logloss:0.05187\n",
      "[63]\tvalidation_0-logloss:0.02513\tvalidation_1-logloss:0.05208\n",
      "[64]\tvalidation_0-logloss:0.02482\tvalidation_1-logloss:0.05347\n",
      "[65]\tvalidation_0-logloss:0.02437\tvalidation_1-logloss:0.05286\n",
      "[66]\tvalidation_0-logloss:0.02399\tvalidation_1-logloss:0.05296\n",
      "[67]\tvalidation_0-logloss:0.02371\tvalidation_1-logloss:0.05280\n",
      "[68]\tvalidation_0-logloss:0.02349\tvalidation_1-logloss:0.05126\n",
      "[69]\tvalidation_0-logloss:0.02296\tvalidation_1-logloss:0.05041\n",
      "[70]\tvalidation_0-logloss:0.02284\tvalidation_1-logloss:0.05070\n",
      "[71]\tvalidation_0-logloss:0.02258\tvalidation_1-logloss:0.05200\n",
      "[72]\tvalidation_0-logloss:0.02228\tvalidation_1-logloss:0.05150\n",
      "[73]\tvalidation_0-logloss:0.02200\tvalidation_1-logloss:0.04969\n",
      "[74]\tvalidation_0-logloss:0.02174\tvalidation_1-logloss:0.04928\n",
      "[75]\tvalidation_0-logloss:0.02153\tvalidation_1-logloss:0.04815\n",
      "[76]\tvalidation_0-logloss:0.02140\tvalidation_1-logloss:0.04844\n",
      "[77]\tvalidation_0-logloss:0.02107\tvalidation_1-logloss:0.04732\n",
      "[78]\tvalidation_0-logloss:0.02096\tvalidation_1-logloss:0.04707\n",
      "[79]\tvalidation_0-logloss:0.02072\tvalidation_1-logloss:0.04666\n",
      "[80]\tvalidation_0-logloss:0.02055\tvalidation_1-logloss:0.04604\n",
      "[81]\tvalidation_0-logloss:0.02035\tvalidation_1-logloss:0.04640\n",
      "[82]\tvalidation_0-logloss:0.02008\tvalidation_1-logloss:0.04652\n",
      "[83]\tvalidation_0-logloss:0.01974\tvalidation_1-logloss:0.04595\n",
      "[84]\tvalidation_0-logloss:0.01948\tvalidation_1-logloss:0.04547\n",
      "[85]\tvalidation_0-logloss:0.01932\tvalidation_1-logloss:0.04417\n",
      "[86]\tvalidation_0-logloss:0.01922\tvalidation_1-logloss:0.04443\n",
      "[87]\tvalidation_0-logloss:0.01905\tvalidation_1-logloss:0.04413\n",
      "[88]\tvalidation_0-logloss:0.01888\tvalidation_1-logloss:0.04420\n",
      "[89]\tvalidation_0-logloss:0.01881\tvalidation_1-logloss:0.04380\n",
      "[90]\tvalidation_0-logloss:0.01873\tvalidation_1-logloss:0.04402\n",
      "[91]\tvalidation_0-logloss:0.01864\tvalidation_1-logloss:0.04429\n",
      "[92]\tvalidation_0-logloss:0.01856\tvalidation_1-logloss:0.04484\n",
      "[93]\tvalidation_0-logloss:0.01848\tvalidation_1-logloss:0.04441\n",
      "[94]\tvalidation_0-logloss:0.01840\tvalidation_1-logloss:0.04469\n",
      "[95]\tvalidation_0-logloss:0.01831\tvalidation_1-logloss:0.04491\n",
      "[96]\tvalidation_0-logloss:0.01824\tvalidation_1-logloss:0.04517\n",
      "[97]\tvalidation_0-logloss:0.01816\tvalidation_1-logloss:0.04538\n",
      "[98]\tvalidation_0-logloss:0.01808\tvalidation_1-logloss:0.04565\n",
      "[99]\tvalidation_0-logloss:0.01801\tvalidation_1-logloss:0.04606\n",
      "[100]\tvalidation_0-logloss:0.01794\tvalidation_1-logloss:0.04631\n",
      "[101]\tvalidation_0-logloss:0.01787\tvalidation_1-logloss:0.04613\n",
      "[102]\tvalidation_0-logloss:0.01780\tvalidation_1-logloss:0.04638\n",
      "[103]\tvalidation_0-logloss:0.01773\tvalidation_1-logloss:0.04596\n",
      "[104]\tvalidation_0-logloss:0.01767\tvalidation_1-logloss:0.04581\n",
      "[105]\tvalidation_0-logloss:0.01760\tvalidation_1-logloss:0.04564\n",
      "[106]\tvalidation_0-logloss:0.01754\tvalidation_1-logloss:0.04590\n",
      "[107]\tvalidation_0-logloss:0.01746\tvalidation_1-logloss:0.04614\n",
      "[108]\tvalidation_0-logloss:0.01739\tvalidation_1-logloss:0.04638\n",
      "[109]\tvalidation_0-logloss:0.01733\tvalidation_1-logloss:0.04599\n",
      "[110]\tvalidation_0-logloss:0.01726\tvalidation_1-logloss:0.04539\n",
      "[111]\tvalidation_0-logloss:0.01719\tvalidation_1-logloss:0.04559\n",
      "[112]\tvalidation_0-logloss:0.01712\tvalidation_1-logloss:0.04522\n",
      "[113]\tvalidation_0-logloss:0.01706\tvalidation_1-logloss:0.04541\n",
      "[114]\tvalidation_0-logloss:0.01699\tvalidation_1-logloss:0.04568\n",
      "[115]\tvalidation_0-logloss:0.01693\tvalidation_1-logloss:0.04594\n",
      "[116]\tvalidation_0-logloss:0.01688\tvalidation_1-logloss:0.04632\n",
      "[117]\tvalidation_0-logloss:0.01681\tvalidation_1-logloss:0.04619\n",
      "[118]\tvalidation_0-logloss:0.01675\tvalidation_1-logloss:0.04579\n",
      "[119]\tvalidation_0-logloss:0.01669\tvalidation_1-logloss:0.04580\n",
      "[120]\tvalidation_0-logloss:0.01663\tvalidation_1-logloss:0.04602\n",
      "[121]\tvalidation_0-logloss:0.01658\tvalidation_1-logloss:0.04589\n",
      "[122]\tvalidation_0-logloss:0.01652\tvalidation_1-logloss:0.04611\n",
      "[123]\tvalidation_0-logloss:0.01646\tvalidation_1-logloss:0.04565\n",
      "[124]\tvalidation_0-logloss:0.01640\tvalidation_1-logloss:0.04526\n",
      "[125]\tvalidation_0-logloss:0.01635\tvalidation_1-logloss:0.04549\n",
      "[126]\tvalidation_0-logloss:0.01630\tvalidation_1-logloss:0.04585\n",
      "[127]\tvalidation_0-logloss:0.01624\tvalidation_1-logloss:0.04572\n",
      "[128]\tvalidation_0-logloss:0.01618\tvalidation_1-logloss:0.04598\n",
      "[129]\tvalidation_0-logloss:0.01613\tvalidation_1-logloss:0.04543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130]\tvalidation_0-logloss:0.01608\tvalidation_1-logloss:0.04531\n",
      "[131]\tvalidation_0-logloss:0.01602\tvalidation_1-logloss:0.04552\n",
      "[132]\tvalidation_0-logloss:0.01597\tvalidation_1-logloss:0.04506\n",
      "[133]\tvalidation_0-logloss:0.01591\tvalidation_1-logloss:0.04532\n",
      "[134]\tvalidation_0-logloss:0.01586\tvalidation_1-logloss:0.04543\n",
      "[135]\tvalidation_0-logloss:0.01581\tvalidation_1-logloss:0.04579\n",
      "[136]\tvalidation_0-logloss:0.01576\tvalidation_1-logloss:0.04581\n",
      "[137]\tvalidation_0-logloss:0.01571\tvalidation_1-logloss:0.04608\n",
      "[138]\tvalidation_0-logloss:0.01566\tvalidation_1-logloss:0.04636\n",
      "오차 행렬\n",
      "[[41  1]\n",
      " [ 1 71]]\n",
      "정확도: 0.9825, 정밀도: 0.9861, 재현율: 0.9861,    F1: 0.9861, AUC:0.9997\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators = 400,learning_rate=round(best['learning_rate'],5),\n",
    "                            max_depth = int(best['max_depth']),min_child_weight = int(best['min_child_weight']),\n",
    "                            colsample_bytree=best['colsample_bytree'])\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "xgb_wrapper.fit(X_tr,y_tr,early_stopping_rounds=50,eval_metric='logloss',eval_set=evals,verbose=True)\n",
    "\n",
    "preds = xgb_wrapper.predict(X_test)\n",
    "pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]\n",
    "\n",
    "get_clf_eval(y_test,preds,pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adaa5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.679030</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>-0.942837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.639898</td>\n",
       "      <td>0.197092</td>\n",
       "      <td>-0.956024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.882734</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>-0.953802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.717072</td>\n",
       "      <td>0.135197</td>\n",
       "      <td>-0.955995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840375</td>\n",
       "      <td>0.124859</td>\n",
       "      <td>-0.951595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829519</td>\n",
       "      <td>0.153391</td>\n",
       "      <td>-0.945016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.862047</td>\n",
       "      <td>0.033687</td>\n",
       "      <td>-0.942808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852322</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>-0.949402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.656299</td>\n",
       "      <td>0.199226</td>\n",
       "      <td>-0.958217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995589</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>-0.940615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.557822</td>\n",
       "      <td>0.101498</td>\n",
       "      <td>-0.958174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.561773</td>\n",
       "      <td>0.194848</td>\n",
       "      <td>-0.956010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.926680</td>\n",
       "      <td>0.038259</td>\n",
       "      <td>-0.942808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753805</td>\n",
       "      <td>0.139652</td>\n",
       "      <td>-0.947209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.653452</td>\n",
       "      <td>0.125109</td>\n",
       "      <td>-0.953802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.951595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.554646</td>\n",
       "      <td>0.021208</td>\n",
       "      <td>-0.938408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.580411</td>\n",
       "      <td>0.183302</td>\n",
       "      <td>-0.958203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.542125</td>\n",
       "      <td>0.119193</td>\n",
       "      <td>-0.953788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.930596</td>\n",
       "      <td>0.087327</td>\n",
       "      <td>-0.951580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.501357</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>-0.958217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.515201</td>\n",
       "      <td>0.176731</td>\n",
       "      <td>-0.956024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.775523</td>\n",
       "      <td>0.170090</td>\n",
       "      <td>-0.958203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.505957</td>\n",
       "      <td>0.068227</td>\n",
       "      <td>-0.953802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.620591</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>-0.955995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.686639</td>\n",
       "      <td>0.160415</td>\n",
       "      <td>-0.953802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802310</td>\n",
       "      <td>0.179006</td>\n",
       "      <td>-0.949416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.717999</td>\n",
       "      <td>0.189430</td>\n",
       "      <td>-0.955981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.684430</td>\n",
       "      <td>0.165839</td>\n",
       "      <td>-0.953817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.615281</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>-0.947209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.501869</td>\n",
       "      <td>0.140173</td>\n",
       "      <td>-0.962603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.634981</td>\n",
       "      <td>0.100550</td>\n",
       "      <td>-0.951580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.662757</td>\n",
       "      <td>0.138956</td>\n",
       "      <td>-0.960396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.707302</td>\n",
       "      <td>0.141573</td>\n",
       "      <td>-0.953788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.734617</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>-0.945016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.527924</td>\n",
       "      <td>0.116199</td>\n",
       "      <td>-0.962603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.057904</td>\n",
       "      <td>-0.947209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.527125</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>-0.962603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600289</td>\n",
       "      <td>0.149478</td>\n",
       "      <td>-0.955995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578658</td>\n",
       "      <td>0.089940</td>\n",
       "      <td>-0.953788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.532491</td>\n",
       "      <td>0.130565</td>\n",
       "      <td>-0.960410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.521982</td>\n",
       "      <td>0.109217</td>\n",
       "      <td>-0.962603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.904720</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>-0.945001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.809984</td>\n",
       "      <td>0.049462</td>\n",
       "      <td>-0.940615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.578009</td>\n",
       "      <td>0.077178</td>\n",
       "      <td>-0.951595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999155</td>\n",
       "      <td>0.097786</td>\n",
       "      <td>-0.940615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.774918</td>\n",
       "      <td>0.157105</td>\n",
       "      <td>-0.951624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973797</td>\n",
       "      <td>0.145913</td>\n",
       "      <td>-0.947209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.869329</td>\n",
       "      <td>0.130088</td>\n",
       "      <td>-0.953831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.641765</td>\n",
       "      <td>0.045023</td>\n",
       "      <td>-0.942794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  learning_rate    losses\n",
       "0        10.0               1.0          0.679030       0.027392 -0.942837\n",
       "1         7.0               2.0          0.639898       0.197092 -0.956024\n",
       "2        16.0               2.0          0.882734       0.110725 -0.953802\n",
       "3        10.0               2.0          0.717072       0.135197 -0.955995\n",
       "4         7.0               1.0          0.840375       0.124859 -0.951595\n",
       "5         7.0               1.0          0.829519       0.153391 -0.945016\n",
       "6        17.0               1.0          0.862047       0.033687 -0.942808\n",
       "7         8.0               1.0          0.852322       0.152206 -0.949402\n",
       "8        17.0               2.0          0.656299       0.199226 -0.958217\n",
       "9        19.0               1.0          0.995589       0.029320 -0.940615\n",
       "10       12.0               1.0          0.557822       0.101498 -0.958174\n",
       "11       17.0               2.0          0.561773       0.194848 -0.956010\n",
       "12        9.0               2.0          0.926680       0.038259 -0.942808\n",
       "13       10.0               1.0          0.753805       0.139652 -0.947209\n",
       "14        8.0               2.0          0.653452       0.125109 -0.953802\n",
       "15       16.0               1.0          0.595886       0.026855 -0.951595\n",
       "16        9.0               2.0          0.554646       0.021208 -0.938408\n",
       "17       16.0               2.0          0.580411       0.183302 -0.958203\n",
       "18       11.0               1.0          0.542125       0.119193 -0.953788\n",
       "19       13.0               2.0          0.930596       0.087327 -0.951580\n",
       "20       20.0               2.0          0.501357       0.175256 -0.958217\n",
       "21       20.0               2.0          0.515201       0.176731 -0.956024\n",
       "22       19.0               2.0          0.775523       0.170090 -0.958203\n",
       "23       14.0               2.0          0.505957       0.068227 -0.953802\n",
       "24       20.0               2.0          0.620591       0.199565 -0.955995\n",
       "25       18.0               2.0          0.686639       0.160415 -0.953802\n",
       "26       14.0               2.0          0.802310       0.179006 -0.949416\n",
       "27       19.0               2.0          0.717999       0.189430 -0.955981\n",
       "28       15.0               2.0          0.684430       0.165839 -0.953817\n",
       "29       18.0               2.0          0.615281       0.065516 -0.947209\n",
       "30       18.0               2.0          0.501869       0.140173 -0.962603\n",
       "31       14.0               2.0          0.634981       0.100550 -0.951580\n",
       "32       17.0               2.0          0.662757       0.138956 -0.960396\n",
       "33       15.0               2.0          0.707302       0.141573 -0.953788\n",
       "34       18.0               2.0          0.734617       0.085394 -0.945016\n",
       "35       12.0               2.0          0.527924       0.116199 -0.962603\n",
       "36        6.0               2.0          0.500034       0.057904 -0.947209\n",
       "37       12.0               2.0          0.527125       0.116718 -0.962603\n",
       "38       11.0               1.0          0.600289       0.149478 -0.955995\n",
       "39       13.0               1.0          0.578658       0.089940 -0.953788\n",
       "40       11.0               2.0          0.532491       0.130565 -0.960410\n",
       "41       12.0               2.0          0.521982       0.109217 -0.962603\n",
       "42       10.0               1.0          0.904720       0.108213 -0.945001\n",
       "43        5.0               2.0          0.809984       0.049462 -0.940615\n",
       "44        8.0               2.0          0.578009       0.077178 -0.951595\n",
       "45       16.0               1.0          0.999155       0.097786 -0.940615\n",
       "46       15.0               2.0          0.774918       0.157105 -0.951624\n",
       "47        9.0               1.0          0.973797       0.145913 -0.947209\n",
       "48       13.0               2.0          0.869329       0.130088 -0.953831\n",
       "49        7.0               2.0          0.641765       0.045023 -0.942794"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "result_df = pd.DataFrame({'max_depth': trial_val.vals['max_depth'],\n",
    "                          'min_child_weight': trial_val.vals['min_child_weight'],\n",
    "                          'colsample_bytree': trial_val.vals['colsample_bytree'],\n",
    "                          'learning_rate': trial_val.vals['learning_rate'],\n",
    "                          'losses': losses\n",
    "                         }\n",
    "                        )\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedad1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
